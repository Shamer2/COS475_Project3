{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVQOKsewCndu"
      },
      "source": [
        "## County Health Rankings\n",
        "\n",
        "This data set is intended to demonstrate how various factors affect the health of counties in the United States, as well as highlight the dramatic range in health from county to county. These factors include access to affordable housing, access to well-paying jobs, education opportunities, and many others. This data is provided as a resource to help identify and address injustices and inequities in the health of counties in the United States.\n",
        "\n",
        "The data set includes statistics on premature death rates, physical health, mental health, low birthweight, adult smoking, and countless others. The data is primarily numeric.\n",
        "\n",
        "Documentation can be found at : https://www.countyhealthrankings.org/sites/default/files/media/document/DataDictionary_2021.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "HRVEv2hyCndx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "\n",
        "from sklearn.impute import SimpleImputer \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from pylab import cm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yltcgOhDCndy"
      },
      "source": [
        "#### Read in the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "KQzpIf4dCndy"
      },
      "outputs": [],
      "source": [
        "url = 'http://www.countyhealthrankings.org/sites/default/files/media/document/analytic_data2021.csv'\n",
        "response = requests.get(url)\n",
        "\n",
        "file_object = io.StringIO(response.content.decode('utf-8'))\n",
        "df = pd.read_csv(file_object,skiprows=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "eycgb5YpCndz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statecode</th>\n",
              "      <th>countycode</th>\n",
              "      <th>fipscode</th>\n",
              "      <th>state</th>\n",
              "      <th>county</th>\n",
              "      <th>year</th>\n",
              "      <th>county_ranked</th>\n",
              "      <th>v001_rawvalue</th>\n",
              "      <th>v001_numerator</th>\n",
              "      <th>v001_denominator</th>\n",
              "      <th>...</th>\n",
              "      <th>v057_rawvalue</th>\n",
              "      <th>v057_numerator</th>\n",
              "      <th>v057_denominator</th>\n",
              "      <th>v057_cilow</th>\n",
              "      <th>v057_cihigh</th>\n",
              "      <th>v058_rawvalue</th>\n",
              "      <th>v058_numerator</th>\n",
              "      <th>v058_denominator</th>\n",
              "      <th>v058_cilow</th>\n",
              "      <th>v058_cihigh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>US</td>\n",
              "      <td>United States</td>\n",
              "      <td>2021</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6906.641094</td>\n",
              "      <td>3854074.0</td>\n",
              "      <td>915437195.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.507502</td>\n",
              "      <td>166582199</td>\n",
              "      <td>328239523</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.192690</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>AL</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2021</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9819.887431</td>\n",
              "      <td>82249.0</td>\n",
              "      <td>13651801.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.516739</td>\n",
              "      <td>2533668</td>\n",
              "      <td>4903185</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.409632</td>\n",
              "      <td>1957932.0</td>\n",
              "      <td>4779736.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1001</td>\n",
              "      <td>AL</td>\n",
              "      <td>Autauga County</td>\n",
              "      <td>2021</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7830.053484</td>\n",
              "      <td>787.0</td>\n",
              "      <td>155765.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.515080</td>\n",
              "      <td>28777</td>\n",
              "      <td>55869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.420022</td>\n",
              "      <td>22921.0</td>\n",
              "      <td>54571.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1003</td>\n",
              "      <td>AL</td>\n",
              "      <td>Baldwin County</td>\n",
              "      <td>2021</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7680.477270</td>\n",
              "      <td>3147.0</td>\n",
              "      <td>600539.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.515096</td>\n",
              "      <td>114987</td>\n",
              "      <td>223234</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.422791</td>\n",
              "      <td>77060.0</td>\n",
              "      <td>182265.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1005</td>\n",
              "      <td>AL</td>\n",
              "      <td>Barbour County</td>\n",
              "      <td>2021</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11476.629416</td>\n",
              "      <td>515.0</td>\n",
              "      <td>69011.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.470793</td>\n",
              "      <td>11622</td>\n",
              "      <td>24686</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.677896</td>\n",
              "      <td>18613.0</td>\n",
              "      <td>27457.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 690 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   statecode  countycode  fipscode state          county  year  county_ranked  \\\n",
              "0          0           0         0    US   United States  2021            NaN   \n",
              "1          1           0      1000    AL         Alabama  2021            NaN   \n",
              "2          1           1      1001    AL  Autauga County  2021            1.0   \n",
              "3          1           3      1003    AL  Baldwin County  2021            1.0   \n",
              "4          1           5      1005    AL  Barbour County  2021            1.0   \n",
              "\n",
              "   v001_rawvalue  v001_numerator  v001_denominator  ...  v057_rawvalue  \\\n",
              "0    6906.641094       3854074.0       915437195.0  ...       0.507502   \n",
              "1    9819.887431         82249.0        13651801.0  ...       0.516739   \n",
              "2    7830.053484           787.0          155765.0  ...       0.515080   \n",
              "3    7680.477270          3147.0          600539.0  ...       0.515096   \n",
              "4   11476.629416           515.0           69011.0  ...       0.470793   \n",
              "\n",
              "   v057_numerator  v057_denominator  v057_cilow  v057_cihigh  v058_rawvalue  \\\n",
              "0       166582199         328239523         NaN          NaN       0.192690   \n",
              "1         2533668           4903185         NaN          NaN       0.409632   \n",
              "2           28777             55869         NaN          NaN       0.420022   \n",
              "3          114987            223234         NaN          NaN       0.422791   \n",
              "4           11622             24686         NaN          NaN       0.677896   \n",
              "\n",
              "   v058_numerator  v058_denominator  v058_cilow  v058_cihigh  \n",
              "0             NaN               NaN         NaN          NaN  \n",
              "1       1957932.0         4779736.0         NaN          NaN  \n",
              "2         22921.0           54571.0         NaN          NaN  \n",
              "3         77060.0          182265.0         NaN          NaN  \n",
              "4         18613.0           27457.0         NaN          NaN  \n",
              "\n",
              "[5 rows x 690 columns]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9KLLrV1Cndz"
      },
      "source": [
        "There are USA aggregate data and state aggregate level data, let's focus only on counties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "EqMmht1KCndz"
      },
      "outputs": [],
      "source": [
        "df = df[df['countycode']!=0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5PVTmxJCnd0"
      },
      "source": [
        "There are 16 Maine counties that we will be taking a close look at later on "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "nID5gMFOCnd0"
      },
      "outputs": [],
      "source": [
        "maine_fipscode = df[(df['state']=='ME')].fipscode\n",
        "maine_county_labels = [' Andr',' Aroo',' Cumb', ' Fran', ' Hanc',' Kenn', ' Knox', ' Linc', ' Oxfo', \n",
        "                       ' Peno', ' Pisc', ' Saga', ' Some', ' Waldo', ' Wash', ' York']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD9ZMTINCnd1"
      },
      "source": [
        "There are a lot of columns we do need. In particular the numerator, denominator, confidence interval, ... columns for forming the raw_values are included. We get rid of them "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "_C1Ikf40Cnd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of CHR variables:  79\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v001_rawvalue</th>\n",
              "      <th>v002_rawvalue</th>\n",
              "      <th>v036_rawvalue</th>\n",
              "      <th>v042_rawvalue</th>\n",
              "      <th>v037_rawvalue</th>\n",
              "      <th>v009_rawvalue</th>\n",
              "      <th>v011_rawvalue</th>\n",
              "      <th>v133_rawvalue</th>\n",
              "      <th>v070_rawvalue</th>\n",
              "      <th>v132_rawvalue</th>\n",
              "      <th>...</th>\n",
              "      <th>v053_rawvalue</th>\n",
              "      <th>v054_rawvalue</th>\n",
              "      <th>v055_rawvalue</th>\n",
              "      <th>v081_rawvalue</th>\n",
              "      <th>v080_rawvalue</th>\n",
              "      <th>v056_rawvalue</th>\n",
              "      <th>v126_rawvalue</th>\n",
              "      <th>v059_rawvalue</th>\n",
              "      <th>v057_rawvalue</th>\n",
              "      <th>v058_rawvalue</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fipscode</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>7830.053484</td>\n",
              "      <td>0.198392</td>\n",
              "      <td>4.501499</td>\n",
              "      <td>4.850214</td>\n",
              "      <td>0.092018</td>\n",
              "      <td>0.198108</td>\n",
              "      <td>0.330</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.691301</td>\n",
              "      <td>...</td>\n",
              "      <td>0.159731</td>\n",
              "      <td>0.198643</td>\n",
              "      <td>0.004761</td>\n",
              "      <td>0.011742</td>\n",
              "      <td>0.001038</td>\n",
              "      <td>0.029909</td>\n",
              "      <td>0.737708</td>\n",
              "      <td>0.008033</td>\n",
              "      <td>0.515080</td>\n",
              "      <td>0.420022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>7680.477270</td>\n",
              "      <td>0.164607</td>\n",
              "      <td>3.647978</td>\n",
              "      <td>4.768294</td>\n",
              "      <td>0.082920</td>\n",
              "      <td>0.185133</td>\n",
              "      <td>0.300</td>\n",
              "      <td>7.8</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.737135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.209780</td>\n",
              "      <td>0.086076</td>\n",
              "      <td>0.007803</td>\n",
              "      <td>0.010661</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.047188</td>\n",
              "      <td>0.832073</td>\n",
              "      <td>0.007085</td>\n",
              "      <td>0.515096</td>\n",
              "      <td>0.422791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>11476.629416</td>\n",
              "      <td>0.298415</td>\n",
              "      <td>5.569267</td>\n",
              "      <td>5.590494</td>\n",
              "      <td>0.113553</td>\n",
              "      <td>0.255308</td>\n",
              "      <td>0.412</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.531668</td>\n",
              "      <td>...</td>\n",
              "      <td>0.196913</td>\n",
              "      <td>0.478287</td>\n",
              "      <td>0.006886</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>0.045248</td>\n",
              "      <td>0.455116</td>\n",
              "      <td>0.018907</td>\n",
              "      <td>0.470793</td>\n",
              "      <td>0.677896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>12172.562382</td>\n",
              "      <td>0.238533</td>\n",
              "      <td>4.894377</td>\n",
              "      <td>5.271114</td>\n",
              "      <td>0.102210</td>\n",
              "      <td>0.230848</td>\n",
              "      <td>0.374</td>\n",
              "      <td>7.6</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.162514</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166696</td>\n",
              "      <td>0.210726</td>\n",
              "      <td>0.004599</td>\n",
              "      <td>0.002143</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.027820</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.003353</td>\n",
              "      <td>0.467313</td>\n",
              "      <td>0.683526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>11096.050649</td>\n",
              "      <td>0.219856</td>\n",
              "      <td>4.986622</td>\n",
              "      <td>5.357809</td>\n",
              "      <td>0.074305</td>\n",
              "      <td>0.229550</td>\n",
              "      <td>0.330</td>\n",
              "      <td>7.9</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.156345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187009</td>\n",
              "      <td>0.015080</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.003199</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.096531</td>\n",
              "      <td>0.867707</td>\n",
              "      <td>0.016185</td>\n",
              "      <td>0.507626</td>\n",
              "      <td>0.899515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 79 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          v001_rawvalue  v002_rawvalue  v036_rawvalue  v042_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001        7830.053484       0.198392       4.501499       4.850214   \n",
              "1003        7680.477270       0.164607       3.647978       4.768294   \n",
              "1005       11476.629416       0.298415       5.569267       5.590494   \n",
              "1007       12172.562382       0.238533       4.894377       5.271114   \n",
              "1009       11096.050649       0.219856       4.986622       5.357809   \n",
              "\n",
              "          v037_rawvalue  v009_rawvalue  v011_rawvalue  v133_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001           0.092018       0.198108          0.330            6.7   \n",
              "1003           0.082920       0.185133          0.300            7.8   \n",
              "1005           0.113553       0.255308          0.412            5.5   \n",
              "1007           0.102210       0.230848          0.374            7.6   \n",
              "1009           0.074305       0.229550          0.330            7.9   \n",
              "\n",
              "          v070_rawvalue  v132_rawvalue  ...  v053_rawvalue  v054_rawvalue  \\\n",
              "fipscode                                ...                                 \n",
              "1001              0.306       0.691301  ...       0.159731       0.198643   \n",
              "1003              0.247       0.737135  ...       0.209780       0.086076   \n",
              "1005              0.280       0.531668  ...       0.196913       0.478287   \n",
              "1007              0.334       0.162514  ...       0.166696       0.210726   \n",
              "1009              0.333       0.156345  ...       0.187009       0.015080   \n",
              "\n",
              "          v055_rawvalue  v081_rawvalue  v080_rawvalue  v056_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001           0.004761       0.011742       0.001038       0.029909   \n",
              "1003           0.007803       0.010661       0.000690       0.047188   \n",
              "1005           0.006886       0.004699       0.002106       0.045248   \n",
              "1007           0.004599       0.002143       0.001161       0.027820   \n",
              "1009           0.006399       0.003199       0.001159       0.096531   \n",
              "\n",
              "          v126_rawvalue  v059_rawvalue  v057_rawvalue  v058_rawvalue  \n",
              "fipscode                                                              \n",
              "1001           0.737708       0.008033       0.515080       0.420022  \n",
              "1003           0.832073       0.007085       0.515096       0.422791  \n",
              "1005           0.455116       0.018907       0.470793       0.677896  \n",
              "1007           0.744083       0.003353       0.467313       0.683526  \n",
              "1009           0.867707       0.016185       0.507626       0.899515  \n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# select all the columns from CHR with raw values\n",
        "## these columns contain the major health-related variables\n",
        "all_cols = df.columns\n",
        "col_names = [i for i in all_cols if 'rawvalue' in i]\n",
        "print ('Number of CHR variables: ',len(col_names))\n",
        "## We include the fipscode column because we want to get only the maine counties out later on\n",
        "col_names.insert(0,\"fipscode\") \n",
        "df_sub = df[col_names]\n",
        "df_sub = df_sub.set_index('fipscode')\n",
        "df_sub.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb3_xt7BCnd2"
      },
      "source": [
        "We get rid of the columns that do not have at least 70% of the rows with values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "BjdAwA4LCnd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               count  percentage\n",
            "v129_rawvalue   1912   60.852960\n",
            "v015_rawvalue   1842   58.625080\n",
            "v149_rawvalue   1836   58.434118\n",
            "v138_rawvalue   1403   44.653087\n",
            "v158_rawvalue   1321   42.043285\n",
            "...              ...         ...\n",
            "v136_rawvalue      0    0.000000\n",
            "v060_rawvalue      0    0.000000\n",
            "v145_rawvalue      0    0.000000\n",
            "v002_rawvalue      0    0.000000\n",
            "v144_rawvalue      0    0.000000\n",
            "\n",
            "[79 rows x 2 columns]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v001_rawvalue</th>\n",
              "      <th>v002_rawvalue</th>\n",
              "      <th>v003_rawvalue</th>\n",
              "      <th>v004_rawvalue</th>\n",
              "      <th>v005_rawvalue</th>\n",
              "      <th>v009_rawvalue</th>\n",
              "      <th>v011_rawvalue</th>\n",
              "      <th>v014_rawvalue</th>\n",
              "      <th>v021_rawvalue</th>\n",
              "      <th>v023_rawvalue</th>\n",
              "      <th>...</th>\n",
              "      <th>v148_rawvalue</th>\n",
              "      <th>v153_rawvalue</th>\n",
              "      <th>v154_rawvalue</th>\n",
              "      <th>v155_rawvalue</th>\n",
              "      <th>v156_rawvalue</th>\n",
              "      <th>v159_rawvalue</th>\n",
              "      <th>v160_rawvalue</th>\n",
              "      <th>v161_rawvalue</th>\n",
              "      <th>v166_rawvalue</th>\n",
              "      <th>v168_rawvalue</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fipscode</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>7830.053484</td>\n",
              "      <td>0.198392</td>\n",
              "      <td>0.130080</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>6650.0</td>\n",
              "      <td>0.198108</td>\n",
              "      <td>0.330</td>\n",
              "      <td>23.764041</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.027281</td>\n",
              "      <td>...</td>\n",
              "      <td>16.562431</td>\n",
              "      <td>0.732906</td>\n",
              "      <td>0.133759</td>\n",
              "      <td>0.42</td>\n",
              "      <td>155.095523</td>\n",
              "      <td>3.126968</td>\n",
              "      <td>2.917280</td>\n",
              "      <td>18.483525</td>\n",
              "      <td>0.805674</td>\n",
              "      <td>0.885166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>7680.477270</td>\n",
              "      <td>0.164607</td>\n",
              "      <td>0.167534</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>3471.0</td>\n",
              "      <td>0.185133</td>\n",
              "      <td>0.300</td>\n",
              "      <td>26.006904</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.027258</td>\n",
              "      <td>...</td>\n",
              "      <td>14.350620</td>\n",
              "      <td>0.752453</td>\n",
              "      <td>0.111740</td>\n",
              "      <td>0.46</td>\n",
              "      <td>133.604842</td>\n",
              "      <td>3.192381</td>\n",
              "      <td>3.037912</td>\n",
              "      <td>19.287661</td>\n",
              "      <td>0.818164</td>\n",
              "      <td>0.908062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>11476.629416</td>\n",
              "      <td>0.298415</td>\n",
              "      <td>0.176732</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>5314.0</td>\n",
              "      <td>0.255308</td>\n",
              "      <td>0.412</td>\n",
              "      <td>37.114262</td>\n",
              "      <td>0.820690</td>\n",
              "      <td>0.037952</td>\n",
              "      <td>...</td>\n",
              "      <td>21.211240</td>\n",
              "      <td>0.608989</td>\n",
              "      <td>0.142707</td>\n",
              "      <td>0.39</td>\n",
              "      <td>102.459673</td>\n",
              "      <td>2.389161</td>\n",
              "      <td>2.008629</td>\n",
              "      <td>11.325918</td>\n",
              "      <td>0.605243</td>\n",
              "      <td>0.732131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>12172.562382</td>\n",
              "      <td>0.238533</td>\n",
              "      <td>0.134299</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>6690.0</td>\n",
              "      <td>0.230848</td>\n",
              "      <td>0.374</td>\n",
              "      <td>37.786446</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.030628</td>\n",
              "      <td>...</td>\n",
              "      <td>20.410337</td>\n",
              "      <td>0.744159</td>\n",
              "      <td>0.087444</td>\n",
              "      <td>0.40</td>\n",
              "      <td>26.588748</td>\n",
              "      <td>2.688775</td>\n",
              "      <td>2.563367</td>\n",
              "      <td>16.955833</td>\n",
              "      <td>0.692062</td>\n",
              "      <td>0.790574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>11096.050649</td>\n",
              "      <td>0.219856</td>\n",
              "      <td>0.179347</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>4440.0</td>\n",
              "      <td>0.229550</td>\n",
              "      <td>0.330</td>\n",
              "      <td>31.239778</td>\n",
              "      <td>0.929256</td>\n",
              "      <td>0.026687</td>\n",
              "      <td>...</td>\n",
              "      <td>21.449131</td>\n",
              "      <td>0.787787</td>\n",
              "      <td>0.078048</td>\n",
              "      <td>0.40</td>\n",
              "      <td>43.309562</td>\n",
              "      <td>2.901714</td>\n",
              "      <td>2.781497</td>\n",
              "      <td>20.022098</td>\n",
              "      <td>0.730081</td>\n",
              "      <td>0.804906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 72 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          v001_rawvalue  v002_rawvalue  v003_rawvalue  v004_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001        7830.053484       0.198392       0.130080       0.000468   \n",
              "1003        7680.477270       0.164607       0.167534       0.000702   \n",
              "1005       11476.629416       0.298415       0.176732       0.000322   \n",
              "1007       12172.562382       0.238533       0.134299       0.000536   \n",
              "1009       11096.050649       0.219856       0.179347       0.000207   \n",
              "\n",
              "          v005_rawvalue  v009_rawvalue  v011_rawvalue  v014_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001             6650.0       0.198108          0.330      23.764041   \n",
              "1003             3471.0       0.185133          0.300      26.006904   \n",
              "1005             5314.0       0.255308          0.412      37.114262   \n",
              "1007             6690.0       0.230848          0.374      37.786446   \n",
              "1009             4440.0       0.229550          0.330      31.239778   \n",
              "\n",
              "          v021_rawvalue  v023_rawvalue  ...  v148_rawvalue  v153_rawvalue  \\\n",
              "fipscode                                ...                                 \n",
              "1001           0.890000       0.027281  ...      16.562431       0.732906   \n",
              "1003           0.880000       0.027258  ...      14.350620       0.752453   \n",
              "1005           0.820690       0.037952  ...      21.211240       0.608989   \n",
              "1007           0.880000       0.030628  ...      20.410337       0.744159   \n",
              "1009           0.929256       0.026687  ...      21.449131       0.787787   \n",
              "\n",
              "          v154_rawvalue  v155_rawvalue  v156_rawvalue  v159_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001           0.133759           0.42     155.095523       3.126968   \n",
              "1003           0.111740           0.46     133.604842       3.192381   \n",
              "1005           0.142707           0.39     102.459673       2.389161   \n",
              "1007           0.087444           0.40      26.588748       2.688775   \n",
              "1009           0.078048           0.40      43.309562       2.901714   \n",
              "\n",
              "          v160_rawvalue  v161_rawvalue  v166_rawvalue  v168_rawvalue  \n",
              "fipscode                                                              \n",
              "1001           2.917280      18.483525       0.805674       0.885166  \n",
              "1003           3.037912      19.287661       0.818164       0.908062  \n",
              "1005           2.008629      11.325918       0.605243       0.732131  \n",
              "1007           2.563367      16.955833       0.692062       0.790574  \n",
              "1009           2.781497      20.022098       0.730081       0.804906  \n",
              "\n",
              "[5 rows x 72 columns]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count and find the percentage of null values and concatenat the results\n",
        "missing = pd.concat([df_sub.isnull().sum(), 100*df_sub.isnull().mean()], axis=1)\n",
        "missing.columns = ['count', 'percentage']\n",
        "smissing = missing.sort_values(by='count', ascending=False)\n",
        "print(smissing)\n",
        "good_cols = smissing[smissing['percentage'] < 30].index\n",
        "good_cols = good_cols.sort_values()\n",
        "df_sub2 = df_sub[good_cols]\n",
        "df_sub2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhaz580gCnd2"
      },
      "source": [
        "I do a demonstration plot that should be helpful for the Assignment 3 work "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "RCzE12wzCnd2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "# a boolean series gets created with true values for maine counties \n",
        "maine_counties = df_sub2.index.isin(maine_fipscode)\n",
        "\n",
        "x_axis = df_sub2[maine_counties]['v036_rawvalue']\n",
        "y_axis = df_sub2[maine_counties]['v001_rawvalue']\n",
        "ax.scatter(x_axis, y_axis)\n",
        "\n",
        "ax.set_xlabel('Poor Physical health days', fontsize=18)\n",
        "ax.set_ylabel('Premature Death', fontsize=16)\n",
        "\n",
        "for i, label in enumerate(maine_county_labels):\n",
        "    plt.annotate(label, (x_axis.iloc[i], y_axis.iloc[i]))\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "prepared_data = pipeline.fit_transform(df_sub2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v001_rawvalue</th>\n",
              "      <th>v002_rawvalue</th>\n",
              "      <th>v003_rawvalue</th>\n",
              "      <th>v004_rawvalue</th>\n",
              "      <th>v005_rawvalue</th>\n",
              "      <th>v009_rawvalue</th>\n",
              "      <th>v011_rawvalue</th>\n",
              "      <th>v014_rawvalue</th>\n",
              "      <th>v021_rawvalue</th>\n",
              "      <th>v023_rawvalue</th>\n",
              "      <th>...</th>\n",
              "      <th>v148_rawvalue</th>\n",
              "      <th>v153_rawvalue</th>\n",
              "      <th>v154_rawvalue</th>\n",
              "      <th>v155_rawvalue</th>\n",
              "      <th>v156_rawvalue</th>\n",
              "      <th>v159_rawvalue</th>\n",
              "      <th>v160_rawvalue</th>\n",
              "      <th>v161_rawvalue</th>\n",
              "      <th>v166_rawvalue</th>\n",
              "      <th>v168_rawvalue</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fipscode</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>-0.247243</td>\n",
              "      <td>-0.054049</td>\n",
              "      <td>-0.101889</td>\n",
              "      <td>-0.205470</td>\n",
              "      <td>1.182201</td>\n",
              "      <td>-0.364320</td>\n",
              "      <td>-0.075332</td>\n",
              "      <td>-0.313281</td>\n",
              "      <td>0.131797</td>\n",
              "      <td>-0.857910</td>\n",
              "      <td>...</td>\n",
              "      <td>0.135078</td>\n",
              "      <td>0.203832</td>\n",
              "      <td>0.705685</td>\n",
              "      <td>-0.109118</td>\n",
              "      <td>0.004158</td>\n",
              "      <td>0.454784</td>\n",
              "      <td>-0.221560</td>\n",
              "      <td>0.004928</td>\n",
              "      <td>0.584326</td>\n",
              "      <td>0.250498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>-0.302948</td>\n",
              "      <td>-0.714412</td>\n",
              "      <td>0.509630</td>\n",
              "      <td>0.461484</td>\n",
              "      <td>-0.653917</td>\n",
              "      <td>-0.676814</td>\n",
              "      <td>-0.577581</td>\n",
              "      <td>-0.141947</td>\n",
              "      <td>-0.024877</td>\n",
              "      <td>-0.859441</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.228653</td>\n",
              "      <td>0.438010</td>\n",
              "      <td>0.075119</td>\n",
              "      <td>0.291640</td>\n",
              "      <td>-0.069242</td>\n",
              "      <td>0.706539</td>\n",
              "      <td>0.189496</td>\n",
              "      <td>0.121032</td>\n",
              "      <td>0.725538</td>\n",
              "      <td>0.616148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>1.110815</td>\n",
              "      <td>1.901005</td>\n",
              "      <td>0.659803</td>\n",
              "      <td>-0.621592</td>\n",
              "      <td>0.410558</td>\n",
              "      <td>1.013361</td>\n",
              "      <td>1.297483</td>\n",
              "      <td>0.706552</td>\n",
              "      <td>-0.954114</td>\n",
              "      <td>-0.138927</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899573</td>\n",
              "      <td>-1.280789</td>\n",
              "      <td>0.961944</td>\n",
              "      <td>-0.409686</td>\n",
              "      <td>-0.175616</td>\n",
              "      <td>-2.384825</td>\n",
              "      <td>-3.317797</td>\n",
              "      <td>-1.028510</td>\n",
              "      <td>-1.681706</td>\n",
              "      <td>-2.193532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>1.369995</td>\n",
              "      <td>0.730548</td>\n",
              "      <td>-0.032999</td>\n",
              "      <td>-0.011500</td>\n",
              "      <td>1.205304</td>\n",
              "      <td>0.424245</td>\n",
              "      <td>0.661300</td>\n",
              "      <td>0.757900</td>\n",
              "      <td>-0.024877</td>\n",
              "      <td>-0.632443</td>\n",
              "      <td>...</td>\n",
              "      <td>0.767864</td>\n",
              "      <td>0.338645</td>\n",
              "      <td>-0.620675</td>\n",
              "      <td>-0.309497</td>\n",
              "      <td>-0.434748</td>\n",
              "      <td>-1.231696</td>\n",
              "      <td>-1.427522</td>\n",
              "      <td>-0.215645</td>\n",
              "      <td>-0.700148</td>\n",
              "      <td>-1.260174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>0.969080</td>\n",
              "      <td>0.365492</td>\n",
              "      <td>0.702493</td>\n",
              "      <td>-0.946490</td>\n",
              "      <td>-0.094244</td>\n",
              "      <td>0.392969</td>\n",
              "      <td>-0.075332</td>\n",
              "      <td>0.257796</td>\n",
              "      <td>0.746833</td>\n",
              "      <td>-0.897957</td>\n",
              "      <td>...</td>\n",
              "      <td>0.938694</td>\n",
              "      <td>0.861341</td>\n",
              "      <td>-0.889766</td>\n",
              "      <td>-0.309497</td>\n",
              "      <td>-0.377639</td>\n",
              "      <td>-0.412154</td>\n",
              "      <td>-0.684242</td>\n",
              "      <td>0.227072</td>\n",
              "      <td>-0.270312</td>\n",
              "      <td>-1.031292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56037</th>\n",
              "      <td>-0.267719</td>\n",
              "      <td>-0.717821</td>\n",
              "      <td>0.038501</td>\n",
              "      <td>-0.412658</td>\n",
              "      <td>-1.189908</td>\n",
              "      <td>-0.677941</td>\n",
              "      <td>-0.527356</td>\n",
              "      <td>0.106786</td>\n",
              "      <td>-1.235965</td>\n",
              "      <td>-0.073688</td>\n",
              "      <td>...</td>\n",
              "      <td>1.188040</td>\n",
              "      <td>0.555074</td>\n",
              "      <td>-0.717269</td>\n",
              "      <td>-0.109118</td>\n",
              "      <td>0.223189</td>\n",
              "      <td>0.942707</td>\n",
              "      <td>1.480645</td>\n",
              "      <td>1.608381</td>\n",
              "      <td>0.972909</td>\n",
              "      <td>0.932438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56039</th>\n",
              "      <td>-2.053617</td>\n",
              "      <td>-1.701282</td>\n",
              "      <td>0.186108</td>\n",
              "      <td>1.547819</td>\n",
              "      <td>-0.821414</td>\n",
              "      <td>-2.032550</td>\n",
              "      <td>-3.758494</td>\n",
              "      <td>-1.414909</td>\n",
              "      <td>-0.103214</td>\n",
              "      <td>-0.861815</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.172476</td>\n",
              "      <td>-1.339862</td>\n",
              "      <td>-0.110378</td>\n",
              "      <td>0.992966</td>\n",
              "      <td>-0.034947</td>\n",
              "      <td>2.040218</td>\n",
              "      <td>2.126417</td>\n",
              "      <td>-0.621607</td>\n",
              "      <td>1.325967</td>\n",
              "      <td>1.315560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56041</th>\n",
              "      <td>-0.153766</td>\n",
              "      <td>-0.625928</td>\n",
              "      <td>0.144279</td>\n",
              "      <td>-0.134209</td>\n",
              "      <td>-1.018945</td>\n",
              "      <td>-0.134607</td>\n",
              "      <td>0.292984</td>\n",
              "      <td>-0.010478</td>\n",
              "      <td>-0.558547</td>\n",
              "      <td>-0.063613</td>\n",
              "      <td>...</td>\n",
              "      <td>0.937087</td>\n",
              "      <td>0.655035</td>\n",
              "      <td>-1.388798</td>\n",
              "      <td>-0.810444</td>\n",
              "      <td>-0.089693</td>\n",
              "      <td>0.519513</td>\n",
              "      <td>1.240118</td>\n",
              "      <td>1.539990</td>\n",
              "      <td>1.598231</td>\n",
              "      <td>0.925219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56043</th>\n",
              "      <td>-0.727284</td>\n",
              "      <td>-0.677250</td>\n",
              "      <td>0.681689</td>\n",
              "      <td>-0.092459</td>\n",
              "      <td>-0.373214</td>\n",
              "      <td>-0.453008</td>\n",
              "      <td>-0.711515</td>\n",
              "      <td>-0.335834</td>\n",
              "      <td>-0.264733</td>\n",
              "      <td>-0.046106</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090670</td>\n",
              "      <td>0.587533</td>\n",
              "      <td>-1.376158</td>\n",
              "      <td>-0.209307</td>\n",
              "      <td>-0.262786</td>\n",
              "      <td>2.500348</td>\n",
              "      <td>2.618103</td>\n",
              "      <td>-0.138195</td>\n",
              "      <td>0.315230</td>\n",
              "      <td>0.448814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56045</th>\n",
              "      <td>-1.726341</td>\n",
              "      <td>-0.631290</td>\n",
              "      <td>0.192321</td>\n",
              "      <td>-0.310909</td>\n",
              "      <td>-1.121177</td>\n",
              "      <td>-0.164944</td>\n",
              "      <td>0.627817</td>\n",
              "      <td>-0.454486</td>\n",
              "      <td>0.131797</td>\n",
              "      <td>-0.751272</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090670</td>\n",
              "      <td>1.369028</td>\n",
              "      <td>-0.229863</td>\n",
              "      <td>-2.012717</td>\n",
              "      <td>-0.303853</td>\n",
              "      <td>1.423244</td>\n",
              "      <td>1.256383</td>\n",
              "      <td>-0.138195</td>\n",
              "      <td>0.157267</td>\n",
              "      <td>1.070397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3142 rows × 72 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          v001_rawvalue  v002_rawvalue  v003_rawvalue  v004_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001          -0.247243      -0.054049      -0.101889      -0.205470   \n",
              "1003          -0.302948      -0.714412       0.509630       0.461484   \n",
              "1005           1.110815       1.901005       0.659803      -0.621592   \n",
              "1007           1.369995       0.730548      -0.032999      -0.011500   \n",
              "1009           0.969080       0.365492       0.702493      -0.946490   \n",
              "...                 ...            ...            ...            ...   \n",
              "56037         -0.267719      -0.717821       0.038501      -0.412658   \n",
              "56039         -2.053617      -1.701282       0.186108       1.547819   \n",
              "56041         -0.153766      -0.625928       0.144279      -0.134209   \n",
              "56043         -0.727284      -0.677250       0.681689      -0.092459   \n",
              "56045         -1.726341      -0.631290       0.192321      -0.310909   \n",
              "\n",
              "          v005_rawvalue  v009_rawvalue  v011_rawvalue  v014_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001           1.182201      -0.364320      -0.075332      -0.313281   \n",
              "1003          -0.653917      -0.676814      -0.577581      -0.141947   \n",
              "1005           0.410558       1.013361       1.297483       0.706552   \n",
              "1007           1.205304       0.424245       0.661300       0.757900   \n",
              "1009          -0.094244       0.392969      -0.075332       0.257796   \n",
              "...                 ...            ...            ...            ...   \n",
              "56037         -1.189908      -0.677941      -0.527356       0.106786   \n",
              "56039         -0.821414      -2.032550      -3.758494      -1.414909   \n",
              "56041         -1.018945      -0.134607       0.292984      -0.010478   \n",
              "56043         -0.373214      -0.453008      -0.711515      -0.335834   \n",
              "56045         -1.121177      -0.164944       0.627817      -0.454486   \n",
              "\n",
              "          v021_rawvalue  v023_rawvalue  ...  v148_rawvalue  v153_rawvalue  \\\n",
              "fipscode                                ...                                 \n",
              "1001           0.131797      -0.857910  ...       0.135078       0.203832   \n",
              "1003          -0.024877      -0.859441  ...      -0.228653       0.438010   \n",
              "1005          -0.954114      -0.138927  ...       0.899573      -1.280789   \n",
              "1007          -0.024877      -0.632443  ...       0.767864       0.338645   \n",
              "1009           0.746833      -0.897957  ...       0.938694       0.861341   \n",
              "...                 ...            ...  ...            ...            ...   \n",
              "56037         -1.235965      -0.073688  ...       1.188040       0.555074   \n",
              "56039         -0.103214      -0.861815  ...      -1.172476      -1.339862   \n",
              "56041         -0.558547      -0.063613  ...       0.937087       0.655035   \n",
              "56043         -0.264733      -0.046106  ...      -0.090670       0.587533   \n",
              "56045          0.131797      -0.751272  ...      -0.090670       1.369028   \n",
              "\n",
              "          v154_rawvalue  v155_rawvalue  v156_rawvalue  v159_rawvalue  \\\n",
              "fipscode                                                               \n",
              "1001           0.705685      -0.109118       0.004158       0.454784   \n",
              "1003           0.075119       0.291640      -0.069242       0.706539   \n",
              "1005           0.961944      -0.409686      -0.175616      -2.384825   \n",
              "1007          -0.620675      -0.309497      -0.434748      -1.231696   \n",
              "1009          -0.889766      -0.309497      -0.377639      -0.412154   \n",
              "...                 ...            ...            ...            ...   \n",
              "56037         -0.717269      -0.109118       0.223189       0.942707   \n",
              "56039         -0.110378       0.992966      -0.034947       2.040218   \n",
              "56041         -1.388798      -0.810444      -0.089693       0.519513   \n",
              "56043         -1.376158      -0.209307      -0.262786       2.500348   \n",
              "56045         -0.229863      -2.012717      -0.303853       1.423244   \n",
              "\n",
              "          v160_rawvalue  v161_rawvalue  v166_rawvalue  v168_rawvalue  \n",
              "fipscode                                                              \n",
              "1001          -0.221560       0.004928       0.584326       0.250498  \n",
              "1003           0.189496       0.121032       0.725538       0.616148  \n",
              "1005          -3.317797      -1.028510      -1.681706      -2.193532  \n",
              "1007          -1.427522      -0.215645      -0.700148      -1.260174  \n",
              "1009          -0.684242       0.227072      -0.270312      -1.031292  \n",
              "...                 ...            ...            ...            ...  \n",
              "56037          1.480645       1.608381       0.972909       0.932438  \n",
              "56039          2.126417      -0.621607       1.325967       1.315560  \n",
              "56041          1.240118       1.539990       1.598231       0.925219  \n",
              "56043          2.618103      -0.138195       0.315230       0.448814  \n",
              "56045          1.256383      -0.138195       0.157267       1.070397  \n",
              "\n",
              "[3142 rows x 72 columns]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepared_data_df = pd.DataFrame(prepared_data)\n",
        "prepared_data_df.index = df_sub2.index\n",
        "prepared_data_df.columns = df_sub2.columns\n",
        "prepared_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = prepared_data_df['v001_rawvalue']\n",
        "X = prepared_data_df.drop('v001_rawvalue', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2552431228232307"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = lin_reg.predict(X_train)\n",
        "lin_mse = mean_squared_error(y_train, y_pred)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[71]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(Dense(n_neurons, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sage\\AppData\\Local\\Temp\\ipykernel_27116\\1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
          ]
        }
      ],
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sage\\anaconda3\\envs\\Project3\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70/71 [============================>.] - ETA: 0s - loss: 0.7242 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 743us/step - loss: 0.7218\n",
            "Epoch 2/100\n",
            "57/71 [=======================>......] - ETA: 0s - loss: 0.3752WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 883us/step - loss: 0.3573\n",
            "Epoch 3/100\n",
            "67/71 [===========================>..] - ETA: 0s - loss: 0.3018WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 784us/step - loss: 0.2997\n",
            "Epoch 4/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.3410WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 670us/step - loss: 0.2648\n",
            "Epoch 5/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.2641WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 710us/step - loss: 0.2397\n",
            "Epoch 6/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.2578WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 604us/step - loss: 0.2228\n",
            "Epoch 7/100\n",
            "63/71 [=========================>....] - ETA: 0s - loss: 0.2133WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 826us/step - loss: 0.2079\n",
            "Epoch 8/100\n",
            "41/71 [================>.............] - ETA: 0s - loss: 0.2109WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 1ms/step - loss: 0.1965\n",
            "Epoch 9/100\n",
            "61/71 [========================>.....] - ETA: 0s - loss: 0.1874WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 841us/step - loss: 0.1870\n",
            "Epoch 10/100\n",
            "68/71 [===========================>..] - ETA: 0s - loss: 0.1800WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 784us/step - loss: 0.1788\n",
            "Epoch 11/100\n",
            "69/71 [============================>.] - ETA: 0s - loss: 0.1718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 769us/step - loss: 0.1716\n",
            "Epoch 12/100\n",
            "59/71 [=======================>......] - ETA: 0s - loss: 0.1736WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 926us/step - loss: 0.1655\n",
            "Epoch 13/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1373WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 712us/step - loss: 0.1603\n",
            "Epoch 14/100\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.1553WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 741us/step - loss: 0.1553\n",
            "Epoch 15/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0949WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 641us/step - loss: 0.1511\n",
            "Epoch 16/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1106WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 677us/step - loss: 0.1470\n",
            "Epoch 17/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0812WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 655us/step - loss: 0.1435\n",
            "Epoch 18/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0784WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 641us/step - loss: 0.1405\n",
            "Epoch 19/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1165WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 641us/step - loss: 0.1374\n",
            "Epoch 20/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.2492WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 709us/step - loss: 0.1346\n",
            "Epoch 21/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0656WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 684us/step - loss: 0.1321\n",
            "Epoch 22/100\n",
            "69/71 [============================>.] - ETA: 0s - loss: 0.1296WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 779us/step - loss: 0.1295\n",
            "Epoch 23/100\n",
            "63/71 [=========================>....] - ETA: 0s - loss: 0.1323WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 812us/step - loss: 0.1271\n",
            "Epoch 24/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1081WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 647us/step - loss: 0.1250\n",
            "Epoch 25/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1610WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 655us/step - loss: 0.1231\n",
            "Epoch 26/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0779WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 641us/step - loss: 0.1212\n",
            "Epoch 27/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1300WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 654us/step - loss: 0.1196\n",
            "Epoch 28/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1851WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 627us/step - loss: 0.1180\n",
            "Epoch 29/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0641WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 642us/step - loss: 0.1164\n",
            "Epoch 30/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0878WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 655us/step - loss: 0.1147\n",
            "Epoch 31/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0618WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 698us/step - loss: 0.1133\n",
            "Epoch 32/100\n",
            "56/71 [======================>.......] - ETA: 0s - loss: 0.1137WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 940us/step - loss: 0.1120\n",
            "Epoch 33/100\n",
            "70/71 [============================>.] - ETA: 0s - loss: 0.1106WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 769us/step - loss: 0.1104\n",
            "Epoch 34/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 712us/step - loss: 0.1092\n",
            "Epoch 35/100\n",
            "58/71 [=======================>......] - ETA: 0s - loss: 0.1120WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 912us/step - loss: 0.1081\n",
            "Epoch 36/100\n",
            "58/71 [=======================>......] - ETA: 0s - loss: 0.1090WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 855us/step - loss: 0.1069\n",
            "Epoch 37/100\n",
            "65/71 [==========================>...] - ETA: 0s - loss: 0.1048WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 812us/step - loss: 0.1057\n",
            "Epoch 38/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0495WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 641us/step - loss: 0.1047\n",
            "Epoch 39/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0638WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 570us/step - loss: 0.1036\n",
            "Epoch 40/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1419WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 698us/step - loss: 0.1027\n",
            "Epoch 41/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1025WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 655us/step - loss: 0.1017\n",
            "Epoch 42/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0837WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 584us/step - loss: 0.1002\n",
            "Epoch 43/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0818WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 584us/step - loss: 0.0999\n",
            "Epoch 44/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0756WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 584us/step - loss: 0.0990\n",
            "Epoch 45/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0793WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 598us/step - loss: 0.0982\n",
            "Epoch 46/100\n",
            "70/71 [============================>.] - ETA: 0s - loss: 0.0977WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 755us/step - loss: 0.0974\n",
            "Epoch 47/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1844WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 606us/step - loss: 0.0966\n",
            "Epoch 48/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0686WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 648us/step - loss: 0.0959\n",
            "Epoch 49/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0593WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 631us/step - loss: 0.0950\n",
            "Epoch 50/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0559WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 627us/step - loss: 0.0945\n",
            "Epoch 51/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0583WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 668us/step - loss: 0.0938\n",
            "Epoch 52/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0895WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 685us/step - loss: 0.0931\n",
            "Epoch 53/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0940WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 690us/step - loss: 0.0924\n",
            "Epoch 54/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0581WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 594us/step - loss: 0.0919\n",
            "Epoch 55/100\n",
            "65/71 [==========================>...] - ETA: 0s - loss: 0.0916WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 784us/step - loss: 0.0913\n",
            "Epoch 56/100\n",
            "66/71 [==========================>...] - ETA: 0s - loss: 0.0927WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 784us/step - loss: 0.0906\n",
            "Epoch 57/100\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.0902WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 741us/step - loss: 0.0902\n",
            "Epoch 58/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0623WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 712us/step - loss: 0.0896\n",
            "Epoch 59/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0809WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 557us/step - loss: 0.0891\n",
            "Epoch 60/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0618WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 649us/step - loss: 0.0886\n",
            "Epoch 61/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0441WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 651us/step - loss: 0.0880\n",
            "Epoch 62/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0471WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 680us/step - loss: 0.0875\n",
            "Epoch 63/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0415WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 631us/step - loss: 0.0867\n",
            "Epoch 64/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0999WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 712us/step - loss: 0.0865\n",
            "Epoch 65/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0727WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 598us/step - loss: 0.0862\n",
            "Epoch 66/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0999WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 659us/step - loss: 0.0856\n",
            "Epoch 67/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 642us/step - loss: 0.0853\n",
            "Epoch 68/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0430WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 627us/step - loss: 0.0847\n",
            "Epoch 69/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0853WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 639us/step - loss: 0.0844\n",
            "Epoch 70/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0688WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 630us/step - loss: 0.0840\n",
            "Epoch 71/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0328WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 633us/step - loss: 0.0833\n",
            "Epoch 72/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0594WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 712us/step - loss: 0.0831\n",
            "Epoch 73/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1002WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 655us/step - loss: 0.0827\n",
            "Epoch 74/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0679WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 698us/step - loss: 0.0824\n",
            "Epoch 75/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0609WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 582us/step - loss: 0.0817\n",
            "Epoch 76/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0516WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 714us/step - loss: 0.0817\n",
            "Epoch 77/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1300WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 591us/step - loss: 0.0814\n",
            "Epoch 78/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0559WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 630us/step - loss: 0.0810\n",
            "Epoch 79/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0616WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 698us/step - loss: 0.0806\n",
            "Epoch 80/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0788WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 713us/step - loss: 0.0802\n",
            "Epoch 81/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0479WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 629us/step - loss: 0.0800\n",
            "Epoch 82/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1187WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 670us/step - loss: 0.0796\n",
            "Epoch 83/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0442WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 698us/step - loss: 0.0793\n",
            "Epoch 84/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0521WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 584us/step - loss: 0.0790\n",
            "Epoch 85/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1036WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 713us/step - loss: 0.0786\n",
            "Epoch 86/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0666WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 591us/step - loss: 0.0784\n",
            "Epoch 87/100\n",
            "67/71 [===========================>..] - ETA: 0s - loss: 0.0793WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 779us/step - loss: 0.0779\n",
            "Epoch 88/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1269WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 635us/step - loss: 0.0777\n",
            "Epoch 89/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0886WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 656us/step - loss: 0.0775\n",
            "Epoch 90/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1132WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 627us/step - loss: 0.0772\n",
            "Epoch 91/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0760WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 705us/step - loss: 0.0770\n",
            "Epoch 92/100\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.0766WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 737us/step - loss: 0.0766\n",
            "Epoch 93/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.1296WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 599us/step - loss: 0.0762\n",
            "Epoch 94/100\n",
            "65/71 [==========================>...] - ETA: 0s - loss: 0.0782WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 812us/step - loss: 0.0759\n",
            "Epoch 95/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.4140WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 684us/step - loss: 0.0757\n",
            "Epoch 96/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0448WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 570us/step - loss: 0.0756\n",
            "Epoch 97/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0838WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 698us/step - loss: 0.0752\n",
            "Epoch 98/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0628WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 584us/step - loss: 0.0750\n",
            "Epoch 99/100\n",
            "70/71 [============================>.] - ETA: 0s - loss: 0.0743WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 755us/step - loss: 0.0748\n",
            "Epoch 100/100\n",
            " 1/71 [..............................] - ETA: 0s - loss: 0.0952WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "71/71 [==============================] - 0s 578us/step - loss: 0.0745\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x189ff268490>"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=100, callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "    'n_hidden': [0, 1, 2, 3],\n",
        "    'n_neurons': np.arange(1, 100),\n",
        "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "# rnd_search_cv.fit(X_train, y_train, epochs=100, callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rnd_search_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sage\\anaconda3\\envs\\Project3\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = build_model(n_hidden=0, n_neurons=22, learning_rate=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "71/71 [==============================] - 0s 648us/step - loss: 1.1082\n",
            "Epoch 2/20\n",
            "71/71 [==============================] - 0s 628us/step - loss: 0.4294\n",
            "Epoch 3/20\n",
            "71/71 [==============================] - 0s 654us/step - loss: 0.2701\n",
            "Epoch 4/20\n",
            "71/71 [==============================] - 0s 639us/step - loss: 0.1966\n",
            "Epoch 5/20\n",
            "71/71 [==============================] - 0s 598us/step - loss: 0.1563\n",
            "Epoch 6/20\n",
            "71/71 [==============================] - 0s 580us/step - loss: 0.1327\n",
            "Epoch 7/20\n",
            "71/71 [==============================] - 0s 627us/step - loss: 0.1169\n",
            "Epoch 8/20\n",
            "71/71 [==============================] - 0s 589us/step - loss: 0.1060\n",
            "Epoch 9/20\n",
            "71/71 [==============================] - 0s 603us/step - loss: 0.0992\n",
            "Epoch 10/20\n",
            "71/71 [==============================] - 0s 612us/step - loss: 0.0940\n",
            "Epoch 11/20\n",
            "71/71 [==============================] - 0s 612us/step - loss: 0.0897\n",
            "Epoch 12/20\n",
            "71/71 [==============================] - 0s 560us/step - loss: 0.0861\n",
            "Epoch 13/20\n",
            "71/71 [==============================] - 0s 577us/step - loss: 0.0837\n",
            "Epoch 14/20\n",
            "71/71 [==============================] - 0s 579us/step - loss: 0.0817\n",
            "Epoch 15/20\n",
            "71/71 [==============================] - 0s 641us/step - loss: 0.0797\n",
            "Epoch 16/20\n",
            "71/71 [==============================] - 0s 575us/step - loss: 0.0784\n",
            "Epoch 17/20\n",
            "71/71 [==============================] - 0s 630us/step - loss: 0.0774\n",
            "Epoch 18/20\n",
            "71/71 [==============================] - 0s 635us/step - loss: 0.0762\n",
            "Epoch 19/20\n",
            "71/71 [==============================] - 0s 633us/step - loss: 0.0752\n",
            "Epoch 20/20\n",
            "71/71 [==============================] - 0s 634us/step - loss: 0.0747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x189f73179d0>"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 712us/step - loss: 0.0705\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.2654992815841323"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mse = model.evaluate(X_val, y_val)\n",
        "rmse = np.sqrt(mse)\n",
        "rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2479459289431052"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = lin_reg.predict(X_test)\n",
        "lin_mse = mean_squared_error(y_test, y_pred)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 0s 577us/step - loss: 0.0712\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.26691364006367463"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mse = model.evaluate(X_test, y_test)\n",
        "rmse = np.sqrt(mse)\n",
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I used what the book did to find the best params for the neural network (Pg 320-22). I'm not certain I did it perfectly, but either way the final results said that 0 hidden layers and 22 neurons was the best. I find this to be somewhat odd because if there are 0 layers then would the number of neurons even matter at that point? Either way though the results were actually pretty good. The linear regression rmse was about 0.248, while the neural network was about 0.268. The linear regression was ultimately better but the neural network came very close to matching it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another little note, I commented the actual search for the best params out becuase it takes around 3 mins to run and I don't want to put you through that if you run this notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "CHR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
